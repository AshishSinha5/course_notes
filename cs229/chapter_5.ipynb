{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kernel Methods\n",
    "\n",
    "## Feature Maps \n",
    "\n",
    "In the case of Linear Regression, we fit a linear function of $x$ w.r.t the training data. What if we wanted to fit a cubic function - \n",
    "$$\n",
    "y = \\theta_0 + \\theta_1x + \\theta_2x^2 + \\theta_3x^3\n",
    "$$\n",
    "\n",
    "Let us define - \n",
    "$$\n",
    "\\phi_(x) = \n",
    "\\left[\\begin{array}{c}\n",
    "1 \\\\\n",
    "x \\\\\n",
    "x^2 \\\\\n",
    "x^3\n",
    "\\end{array}\\right]\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\therefore y = \\theta^T\\phi(x)\n",
    "$$\n",
    "\n",
    "Terminology - \n",
    "- Attribute - The \"original\" input value to the model. e.g. $x$ - living area.\n",
    "- Features - Mapped set of values of the original attribute.\n",
    "- Feature Map - $\\phi$\n",
    "\n",
    "## LMS (least mean squares) with features\n",
    "\n",
    "In the case of set of available features via a feature map $\\phi : \\mathcal{R}^d \\rightarrow \\mathcal{R}^p$, we calculate the update rule by replacing all the occurrences of $x^{(i)}$ with $\\phi(x^{(i)})$ in the following formula - \n",
    "$$\n",
    "\\theta := \\theta + \\alpha\\Sigma_{i=1}^{n}(y^{(i)} - \\theta^Tx^{(i)})x^{(i)}\n",
    "$$\n",
    "\n",
    "$\\therefore$ We have the following update rule - \n",
    "$$\n",
    "\\theta := \\theta + \\alpha\\Sigma_{i=1}^{n}(y^{(i)} - \\theta^T\\phi(x^{(i)}))\\phi(x^{(i)})\n",
    "$$\n",
    "\n",
    "## LMS with Kernel Trick\n",
    "\n",
    "The time complexity of the gradient descent algorithm increases exponentially with the increase in the dimensionality of the parameter vector. For e.g. let $\\phi(x)$ contain all the monomials pf $x$ of the order $\\leq$ 4.  Assuming $d = 3$, $\\phi(x)$ would have the following tasks - \n",
    "\n",
    "$$\n",
    "\\phi = \\begin{bmatrix}\n",
    "1 \\\\\n",
    "x_1 \\\\\n",
    "x_2 \\\\\n",
    ". \\\\\n",
    ". \\\\\n",
    "x_1x_2 \\\\\n",
    ". \\\\\n",
    ". \\\\\n",
    "x_1^2x_2^2 \\\\\n",
    ". \\\\\n",
    ". \\\\\n",
    "x_1x_2x_3^2\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The dimension of the $\\phi$ would be of the order of $d^4$, as the size of the $\\theta$ is itself of the order $d^4$, we may need to store the $\\theta_i \\in \\theta$. The **kernel trick** helps us in improve the runtime where we do not need to explicitly store the $\\theta$ for each update. \n",
    "\n",
    "The update rule is given by - \n",
    "\n",
    "$$\n",
    "\\theta := \\theta + \\alpha\\Sigma_{i=1}^{n}(y^{(i)} - \\theta^T\\phi(x^{(i)}))\\phi(x^{(i)})\n",
    "$$\n",
    "\n",
    "Initialize the $\\theta = 0$, $\\therefore$ is can be inductively shown that for some set $\\beta_i \\in \\mathcal{R}$ it can be shown that - \n",
    "$$\n",
    "\\theta = \\Sigma_{i=1}^n \\beta_i \\phi(x^{(i)})\n",
    "$$ \n",
    "\n",
    "Putting this in the update rule - \n",
    "\n",
    "$$ \\begin{align*}\n",
    "\\theta &:= \\Sigma_{i=1}^n \\beta_i \\phi(x^{(i)}) + \\alpha\\Sigma_{i=1}^{n}(y^{(i)} - \\theta^T\\phi(x^{(i)}))\\phi(x^{(i)}) \\\\ \n",
    "&:= \\Sigma_{i=1}^n(\\beta_i + \\alpha(y^{(i)} - \\theta^T\\phi(x^{(i)})))\\phi(x^{(i)})\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "$\\therefore$ we have an update rule for the $\\beta$\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\beta_i &:= \\beta_i + \\alpha(y^{(i)} - \\theta^{(T)}\\phi(x^{(i)})) \\\\ \n",
    "&:= \\beta_i + \\alpha(y^{(i)} - \\Sigma_{j=1}^n \\beta_j \\phi(x^{(j)})^T\\phi(x^{(i)})) \\\\ \n",
    "&:=  \\beta_i + \\alpha(y^{(i)} - \\Sigma_{j=1}^n \\beta_j <\\phi(x^{(j)}), \\phi(x^{(i)})>\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Where $<.,.>$ is the dot product operator. This dot product can be calculated efficiently.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
