{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization and Model Selection\n",
    "\n",
    "Regularization is a technique in machine learning to control model complexity and prevent overfitting. Typically involves adding an additional term, called a regularizer and denoted by $R(\\theta)$, to the training loss/cost function.\n",
    "$$\n",
    "J_{\\lambda}(\\theta) = J(\\theta) + \\lambda R(\\theta)\n",
    "$$\n",
    "- $J_{\\lambda}$ - regularized loss\n",
    "- $\\lambda \\geq 0$ - regularization parameter\n",
    "\n",
    "Here, $R(\\theta)$ is typically some measure of complexity of model $\\theta$, $\\therefore$ by using a regularized cost function we aim to find the model that fits both the data (minimize $J(\\theta)$) and has smaller complexity i.e. $R(\\theta)$.\n",
    "\n",
    "The regularization parameter acts as a tradeoff between the two objectives - \n",
    "\n",
    "|$\\lambda$| Implication|\n",
    "|-------|--|\n",
    "|$\\lambda = 0$| Unregularized loss function i.e. minimizing only the $J(\\theta)$.|\n",
    "|$\\lambda \\approx 0$| Again effectively minimizing the loss function $J(\\theta)$ with regularization term acting as a tie breaker when multiple solutions with similar loss values exists.|\n",
    "|$\\lambda >> 0$|The original loss is not effective anymore leading to high bias of the model.|\n",
    "\n",
    "We have stated that $R(\\theta)$ is the regularization term measuring the complexity of the model. But we have not explicitly defined the term. Their are many ways of measuring the regularization of the model - \n",
    "- Number of parameters - Larger number of parameters, larger the regularization. But this is very vague since the model can have a large number of parameters but most of them can be non-zero.\n",
    "- Norm of parameters - Typically $L_i$ norms are used to measure the complexity.\n",
    "- Lipschitzness/Smoothness of the curve.\n",
    "\n",
    "The most common way to regularize the model is to add the norm of the parameters to the model. Commonly used norms are - \n",
    "- L0 norm - $R(\\theta) = \\frac{1}{2}||\\theta||_0$ - Number of non-zero parameters in the model. One main drawback of this type of regularization is that it is not differentiable.\n",
    "- L1 norm - $R(\\theta) = \\frac{1}{2}||\\theta||_1$ - Sum of absolute values of the parameters. A good surrogate for the L0 norm as it is differentiable and is *convex approximation for the L0 norm*. [read more here](https://www.cs.utep.edu/vladik/2013/tr13-18.pdf)\n",
    "\n",
    "The L1 and L0 norm deal with the sparsity of the model and often used for the feature selection procedure. If we want to use all the features of the given model but control the magnitude of contribution for each model, we can use the L2 norm.\n",
    "- L2 norm - $R(\\theta) = \\frac{1}{2}||\\theta||_2^2$ - Sum of squares of the parameters. This regularization technique is referred to as weight decay as it decays the weight parameter $\\theta$ by a factor of $1 - \\eta\\lambda$ - \n",
    "$$\\begin{align*}\n",
    "\\theta \\leftarrow \\theta - \\eta \\nabla J_{\\lambda}(\\theta) &=  \\theta - \\eta(\\lambda \\theta  - \\nabla J(\\theta)) \\\\\n",
    "&= (1 - \\eta\\lambda)\\theta - \\eta\\nabla J(\\theta)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Where - \n",
    "- $\\eta$ - learning rate\n",
    "- $\\lambda$ - regularization parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
